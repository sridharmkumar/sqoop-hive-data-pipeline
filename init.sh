#!/bin/bash
read -p "Enter MySQL DB username: " username

#check if username is not empty string
if [ -z $username ]
then
  echo "username cannot be empty string, exiting ..."
  exit
fi

read -sp "Enter MySQL DB password: " password
printf "\n"

read -p "Enter HDFS path : " hdfspath

#check if path is not empty string
if [ -z $hdfspath ]
then
  echo "path cannot be empty string, exiting ..."
  exit
fi

echo "Starting Initialization ..."
mysql -u"$username" -p"$password" < sqlscript.sql

echo -n "$password" > secretkey
hadoop fs -mkdir -p "$hdfspath/pike/conf"
hadoop fs -put secretkey "$hdfspath/pike/conf/secretkey"

sqoop import --connect jdbc:mysql://localhost/pike --query 'select * from web_customer where $CONDITIONS limit 1' --username "$username" --password-file "$hdfspath/pike/conf/secretkey" -m 1 --as-avrodatafile --target-dir "$hdfspath/pike/tmp"

hadoop fs -mkdir "$hdfspath/pike/schema"
hadoop fs -put AutoGeneratedSchema.avsc "$hdfspath/pike/schema"
hadoop fs -rmr "$hdfspath/pike/tmp"

sqoop job --create customer_import -- import --connect jdbc:mysql://localhost/pike --username "$username" --password-file "$hdfspath/pike/conf/secretkey" -table web_customer -m 1 --target-dir "$hdfspath/pike/staging" --incremental append --check-column id --last-value 0 --as-avrodatafile

hive -e "create database pike;use pike;create external table customer_web partitioned by(year string,month string,day string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS AVRO LOCATION '$hdfspath/pike/target'  TBLPROPERTIES ('avro.schema.url'='$hdfspath/pike/schema/AutoGeneratedSchema.avsc');"

sed -i  "2 i hdfspath=$hdfspath" execute.sh

echo "Completed Initialization ..."
